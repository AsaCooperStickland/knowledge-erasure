Engineering 
* [ ] Detailed raw results in a shared google sheet
* [ ] More models
  * [ ] text-bison-001 and chat-bison-001
  * [ ] Cohere
  * [ ] AI21Labs
* [ ] More reasoning datasets
  * [ ] Arc-c
  * [ ] Commonsense
  * [x] Theorem proving 
  * [x] Coding 
  * [ ] API Call
    * [API Bank](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank)
    * [ToolBench](https://github.com/OpenBMB/ToolBench)
  * [ ] proofbank, entailment bank -- maybe sweep all Aristo datasets then see how LLM works 
* [ ] Make this prompt and the associating prompts as a huggingface dataset
* [ ] Draw a figure about model scale v.s. reasoning accuracy 
* [ ] Add Alpaca and Vacuna 
* [ ] Test LLaMA on BBH
* [ ] Fancy tricks in prompt engineering 
* [x] Add smaller LLaMA
* [x] Add Flan-T5

Research
* [ ] Decoding space 
  * [ ] Do larger models have "larger" decoding space than smaller models? 
  * [ ] Can instruction finetuning "closes" some meaningful/ reasonable decoding path? Can it "open" new decoding paths? 
* [ ] Advanced prompt engineering
  * [ ] Planning and deductive prompting 
  * [x] Dialog in-context learning
* [ ] CoT prompt engineering documentation, including 
  * Stable: 
    * [ ] complexity based prompting
  * Test: 
    * [ ] concise prompting
    * [ ] emphasizing important steps 