The commands below run Claude models on the MMLU dataset. 
The engine 'claude-v1.3' is the full capacity one, whereas 'claude-instant-v1.0' is the lightweight version.
You can also specify prompt type to either 'single' or 'multiple'. 'single' refers to a group of demonstration questions with answers being supplied as a part of the prompt, whereas 'multiple' refers to having rounds of conversations for demonstration in the prompt.

```bash
cd MMLU
mkdir outputs
API_KEY=<your_api_key>
python run_mmlu_claude.py --anthropic_key=${API_KEY} --engine=claude-v1.3 --prompt_type='multiple'
```
The commands below run open-resource models on the MMLU dataset (answer-only setting). Now, we has evaluated the LLaMA-65B and Falcon-40B.
The data and the prompts come from the [MMLU original repo](https://github.com/hendrycks/test). 

```bash
LLAMA_CKPT_DIR=<path to model checkpoints>
PARAM_SIZE=65 # 7, 13, 33, 65
MODEL_TYPE=llama # ["llama", "falcon"] 
python run_mmlu_open_source.py --ckpt_dir ${LLAMA_CKPT_DIR} --param_size ${PARAM_SIZE} --model_type ${MODEL_TYPE}
```

Below is the evaluation results of LLaMA-65B and Falcon-40B on each subtask.

|Task|LLaMA-65B|Falcon-40B|LLaMA-65B (Bug)|
|:----:|:----:|:----:|:----:|
|Average|0.6364|0.4908|0.6144|
|abstract_algebra|0.3000|0.2600|0.3000|
|anatomy|0.5778|0.5407|0.5778|
|astronomy|0.7303|0.4868|0.7303|
|business_ethics|0.5900|0.5800|0.5900|
|clinical_knowledge|0.6604|0.5283|0.6604|
|college_biology|0.6944|0.5069|0.6944|
|college_chemistry|0.4800|0.3300|0.4800|
|college_computer_science|0.4700|0.5000|0.4700|
|college_mathematics|0.3500|0.3600|0.3500|
|college_medicine|0.5376|0.4220|0.5376|
|college_physics|0.3529|0.2255|0.3529|
|computer_security|0.8000|0.4600|0.8000|
|conceptual_physics|0.5830|0.4128|0.5830|
|econometrics|0.3947|0.2544|0.3947|
|electrical_engineering|0.5517|0.4966|0.5517|
|elementary_mathematics|0.3968|0.2910|0.3968|
|formal_logic|0.4365|0.2302|0.4365|
|global_facts|0.3800|0.3800|0.3800|
|high_school_biology|0.7452|0.5903|0.7452|
|high_school_chemistry|0.4089|0.3448|0.4089|
|high_school_computer_science|0.6900|0.4400|0.6900|
|high_school_european_history|0.7939|0.5152|0.0000|
|high_school_geography|0.7879|0.6465|0.7879|
|high_school_government_and_politics|0.8808|0.7047|0.8808|
|high_school_macroeconomics|0.6564|0.5000|0.6590|
|high_school_mathematics|0.3444|0.2630|0.3407|
|high_school_microeconomics|0.6807|0.4538|0.6765|
|high_school_physics|0.3642|0.3046|0.3709|
|high_school_psychology|0.8257|0.6349|0.8257|
|high_school_statistics|0.6157|0.4352|0.6204|
|high_school_us_history|0.8333|0.5882|0.0000|
|high_school_world_history|0.8354|0.5485|0.8354|
|human_aging|0.6726|0.6502|0.6726|
|human_sexuality|0.7786|0.6336|0.7786|
|international_law|0.8182|0.6777|0.8182|
|jurisprudence|0.7407|0.5926|0.7500|
|logical_fallacies|0.7730|0.5951|0.7730|
|machine_learning|0.4732|0.3036|0.4732|
|management|0.8252|0.7184|0.8252|
|marketing|0.8718|0.7564|0.8718|
|medical_genetics|0.6800|0.6300|0.6800|
|miscellaneous|0.8135|0.6628|0.8135|
|moral_disputes|0.7370|0.5636|0.7370|
|moral_scenarios|0.4782|0.2804|0.4771|
|nutrition|0.6895|0.5817|0.6895|
|philosophy|0.7363|0.5756|0.7363|
|prehistory|0.7407|0.5710|0.7377|
|professional_accounting|0.4858|0.4326|0.4858|
|professional_law|0.4980|0.3690|0.4935|
|professional_medicine|0.6213|0.4265|0.6176|
|professional_psychology|0.6667|0.4902|0.6650|
|public_relations|0.7545|0.6273|0.7545|
|security_studies|0.7184|0.5633|0.7224|
|sociology|0.8109|0.7114|0.8109|
|us_foreign_policy|0.8800|0.7500|0.8800|
|virology|0.5301|0.4157|0.5301|
|world_religions|0.8129|0.7719|0.8129|


