Pretrained model v.s. pretrained model; instruction-tuned model v.s. instruction-tuned model
* Typically, instruction tuning improves the overall performance of the pretrained checkpoints. It is unfair to compare a pretrained model to another instruction-tuned model

Some results:
* On GSM8K, gpt-3.5-turbo significantly improves over text-davinci-003. It is unclear how OpenAI achieves this
  * GSM8K measures daily life math reasoning -- this is consistent with the release log "improved math reasoning" also improved user experience
* text-davinci-002 v.s. text-davinci-003
  * text-davinci-002 has better CoT than AO; 
  * text-davinci-003 has equally good CoT as AO. 
  * On MMLU, CoT is slightly worse than AO but not much. 
  * Questions can be solved by AO can also be solved by CoT; 
  * Questions can be solved by CoT may not be solved by AO. 
* Pretraining may need to include more natural science data like the Galactica approach to improve reasoning